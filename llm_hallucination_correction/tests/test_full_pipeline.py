#!/usr/bin/env python3
"""
å®Œæ•´æµç¨‹æµ‹è¯•è„šæœ¬ - å¤§è¯­è¨€æ¨¡å‹å¹»è§‰æ£€æµ‹ä¸çº æ­£ç³»ç»Ÿ

æœ¬è„šæœ¬æµ‹è¯•ä»æ„å›¾åˆ†ç±»åˆ°ç­”æ¡ˆçº æ­£çš„å®Œæ•´æµç¨‹ï¼ŒéªŒè¯ç³»ç»Ÿå„æ¨¡å—çš„é›†æˆåŠŸèƒ½ã€‚
"""

import os
import sys
import yaml
import json
import pytest
import asyncio
from datetime import datetime
from unittest.mock import Mock, patch, MagicMock

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.orchestrator import EvidenceEnhancedCorrectionOrchestrator
from src.llm_client import LLMAdapter
from src.retriever import VectorRetriever
from src.intent_classifier import IntentClassifier
from src.claim_extractor import ClaimExtractor
from src.evidence_verifier import EvidenceVerifier
from src.corrector import IntentAwareCorrector

class TestFullPipeline:
    """å®Œæ•´æµç¨‹æµ‹è¯•ç±»"""
    
    @classmethod
    def setup_class(cls):
        """æµ‹è¯•ç±»åˆå§‹åŒ–"""
        print("\n" + "="*60)
        print("ğŸ§ª å¼€å§‹å®Œæ•´æµç¨‹æµ‹è¯•")
        print("="*60)
        
        # åŠ è½½æµ‹è¯•é…ç½®
        cls.test_config = cls._load_test_config()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®ç›®å½•
        os.makedirs('tests/test_data', exist_ok=True)
        
    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•å‰çš„è®¾ç½®"""
        self.start_time = datetime.now()
        print(f"\nâ° å¼€å§‹æµ‹è¯•: {self._testMethodName}")
    
    def teardown_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•åçš„æ¸…ç†"""
        duration = (datetime.now() - self.start_time).total_seconds()
        print(f"âœ… æµ‹è¯•å®Œæˆ: {self._testMethodName} (è€—æ—¶: {duration:.2f}s)")
    
    @staticmethod
    def _load_test_config():
        """åŠ è½½æµ‹è¯•é…ç½®"""
        config_path = 'config/test_config.yaml'
        if not os.path.exists(config_path):
            # åˆ›å»ºåŸºç¡€æµ‹è¯•é…ç½®
            test_config = {
                'llm': {
                    'provider': 'mock',
                    'api_key': 'test_key',
                    'model': 'test-model',
                    'temperature': 0.1,
                    'max_tokens': 500
                },
                'vector_db': {
                    'embedding_model': 'BAAI/bge-base-en',
                    'db_path': 'tests/test_data/vector_db',
                    'collection_name': 'test_knowledge_base'
                },
                'retrieval': {
                    'similarity_threshold': 0.7,
                    'max_retrieved_docs': 5
                },
                'verification': {
                    'confidence_threshold': 0.8,
                    'max_verification_attempts': 3
                },
                'intent': {
                    'supported_intents': ['äº‹å®æŸ¥è¯¢', 'æ¯”è¾ƒæŸ¥è¯¢', 'æ–¹æ³•æŸ¥è¯¢', 'è§‚ç‚¹æŸ¥è¯¢'],
                    'default_intent': 'äº‹å®æŸ¥è¯¢'
                }
            }
            
            os.makedirs('config', exist_ok=True)
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(test_config, f, default_flow_style=False)
        
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _create_mock_llm_response(self, response_text: str):
        """åˆ›å»ºæ¨¡æ‹ŸLLMå“åº”"""
        return {
            'text': response_text,
            'usage': {
                'prompt_tokens': 100,
                'completion_tokens': 200,
                'total_tokens': 300
            },
            'model': 'test-model',
            'finish_reason': 'stop'
        }
    
    def _setup_mock_llm(self):
        """è®¾ç½®æ¨¡æ‹ŸLLM"""
        mock_llm = Mock(spec=LLMAdapter)
        
        # æ¨¡æ‹Ÿæ„å›¾åˆ†ç±»å“åº”
        mock_llm.call_with_retry.return_value = self._create_mock_llm_response('äº‹å®æŸ¥è¯¢')
        
        return mock_llm
    
    def test_01_system_initialization(self):
        """æµ‹è¯•ç³»ç»Ÿåˆå§‹åŒ–"""
        print("ğŸ”§ æµ‹è¯•ç³»ç»Ÿåˆå§‹åŒ–...")
        
        with patch('src.llm_client.LLMAdapter') as mock_llm_class:
            mock_llm_instance = self._setup_mock_llm()
            mock_llm_class.return_value = mock_llm_instance
            
            # åˆå§‹åŒ–ç³»ç»Ÿ
            orchestrator = EvidenceEnhancedCorrectionOrchestrator(self.test_config)
            
            # éªŒè¯ç³»ç»ŸçŠ¶æ€
            status = orchestrator.get_system_status()
            assert status['components_initialized'] == True
            assert 'timestamp' in status
            print("âœ… ç³»ç»Ÿåˆå§‹åŒ–éªŒè¯é€šè¿‡")
    
    def test_02_intent_classification(self):
        """æµ‹è¯•æ„å›¾åˆ†ç±»åŠŸèƒ½"""
        print("ğŸ¯ æµ‹è¯•æ„å›¾åˆ†ç±»...")
        
        test_cases = [
            {
                'query': 'ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ',
                'expected_intent': 'äº‹å®æŸ¥è¯¢',
                'description': 'äº‹å®æ€§æŸ¥è¯¢'
            },
            {
                'query': 'Pythonå’ŒJavaå“ªä¸ªæ›´å¥½ï¼Ÿ',
                'expected_intent': 'æ¯”è¾ƒæŸ¥è¯¢', 
                'description': 'æ¯”è¾ƒæ€§æŸ¥è¯¢'
            },
            {
                'query': 'å¦‚ä½•å­¦ä¹ æ·±åº¦å­¦ä¹ ï¼Ÿ',
                'expected_intent': 'æ–¹æ³•æŸ¥è¯¢',
                'description': 'æ–¹æ³•æ€§æŸ¥è¯¢'
            },
            {
                'query': 'å¤§å®¶å¯¹äººå·¥æ™ºèƒ½çš„çœ‹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ',
                'expected_intent': 'è§‚ç‚¹æŸ¥è¯¢',
                'description': 'è§‚ç‚¹æ€§æŸ¥è¯¢'
            }
        ]
        
        with patch('src.llm_client.LLMAdapter') as mock_llm_class:
            mock_llm_instance = Mock()
            
            for i, test_case in enumerate(test_cases, 1):
                # è®¾ç½®å½“å‰æµ‹è¯•ç”¨ä¾‹çš„æ¨¡æ‹Ÿå“åº”
                mock_llm_instance.call_with_retry.return_value = self._create_mock_llm_response(
                    test_case['expected_intent']
                )
                mock_llm_class.return_value = mock_llm_instance
                
                # æµ‹è¯•æ„å›¾åˆ†ç±»
                classifier = IntentClassifier(mock_llm_instance, self.test_config['intent'])
                detected_intent = classifier.classify_intent(test_case['query'])
                
                assert detected_intent == test_case['expected_intent']
                print(f"âœ… æµ‹è¯•ç”¨ä¾‹ {i}: {test_case['description']} - é€šè¿‡")
    
    def test_03_claim_extraction(self):
        """æµ‹è¯•å£°æ˜æå–åŠŸèƒ½"""
        print("ğŸ” æµ‹è¯•å£°æ˜æå–...")
        
        test_texts = [
            {
                'input': 'Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumåœ¨1991å¹´åˆ›å»ºã€‚å®ƒå…·æœ‰ç®€å•æ˜“å­¦çš„è¯­æ³•ã€‚',
                'expected_claims': 3,
                'description': 'å¤šäº‹å®æ–‡æœ¬'
            },
            {
                'input': 'æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ã€‚',
                'expected_claims': 1, 
                'description': 'å•äº‹å®æ–‡æœ¬'
            },
            {
                'input': 'æ·±åº¦å­¦ä¹ åŸºäºç¥ç»ç½‘ç»œï¼Œèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ ç‰¹å¾è¡¨ç¤ºï¼Œåœ¨å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­è¡¨ç°å‡ºè‰²ã€‚',
                'expected_claims': 3,
                'description': 'å¤æ‚æŠ€æœ¯æè¿°'
            }
        ]
        
        with patch('src.llm_client.LLMAdapter') as mock_llm_class:
            mock_llm_instance = Mock()
            
            for i, test_case in enumerate(test_texts, 1):
                # æ¨¡æ‹Ÿå£°æ˜æå–å“åº”
                mock_response = """
                [CLAIM_1]: Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€
                [CLAIM_2]: Pythonç”±Guido van Rossumåˆ›å»º  
                [CLAIM_3]: Pythonåœ¨1991å¹´åˆ›å»º
                [CLAIM_4]: Pythonå…·æœ‰ç®€å•æ˜“å­¦çš„è¯­æ³•
                """
                mock_llm_instance.call_with_retry.return_value = self._create_mock_llm_response(mock_response)
                mock_llm_class.return_value = mock_llm_instance
                
                # æµ‹è¯•å£°æ˜æå–
                extractor = ClaimExtractor(mock_llm_instance)
                claims = extractor.extract_claims(test_case['input'])
                
                assert len(claims) >= test_case['expected_claims']
                assert all('text' in claim for claim in claims)
                assert all('confidence' in claim for claim in claims)
                print(f"âœ… æµ‹è¯•ç”¨ä¾‹ {i}: {test_case['description']} - æå–åˆ° {len(claims)} ä¸ªå£°æ˜")
    
    def test_04_evidence_retrieval(self):
        """æµ‹è¯•è¯æ®æ£€ç´¢åŠŸèƒ½"""
        print("ğŸ“š æµ‹è¯•è¯æ®æ£€ç´¢...")
        
        # æ¨¡æ‹Ÿå‘é‡æ•°æ®åº“
        with patch('src.retriever.SentenceTransformer') as mock_embedding, \
             patch('src.retriever.chromadb') as mock_chroma:
            
            # è®¾ç½®æ¨¡æ‹ŸåµŒå…¥æ¨¡å‹
            mock_embedding_instance = Mock()
            mock_embedding_instance.encode.return_value = [[0.1, 0.2, 0.3]]  # æ¨¡æ‹ŸåµŒå…¥å‘é‡
            mock_embedding.return_value = mock_embedding_instance
            
            # è®¾ç½®æ¨¡æ‹Ÿå‘é‡æ•°æ®åº“
            mock_collection = Mock()
            mock_collection.query.return_value = {
                'documents': [['æ¨¡æ‹Ÿè¯æ®æ–‡æœ¬1', 'æ¨¡æ‹Ÿè¯æ®æ–‡æœ¬2']],
                'metadatas': [[{'source': 'wiki'}, {'source': 'paper'}]],
                'distances': [[0.1, 0.2]]
            }
            mock_client = Mock()
            mock_client.get_collection.return_value = mock_collection
            mock_chroma.PersistentClient.return_value = mock_client
            
            # åˆå§‹åŒ–æ£€ç´¢å™¨
            retriever = VectorRetriever(self.test_config['vector_db'])
            
            # æµ‹è¯•æ£€ç´¢
            query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
            results = retriever.search(query, n_results=2)
            
            assert len(results) == 2
            assert all('text' in result for result in results)
            assert all('source' in result for result in results)
            assert all('similarity' in result for result in results)
            print("âœ… è¯æ®æ£€ç´¢åŠŸèƒ½éªŒè¯é€šè¿‡")
    
    def test_05_claim_verification(self):
        """æµ‹è¯•å£°æ˜éªŒè¯åŠŸèƒ½"""
        print("âœ… æµ‹è¯•å£°æ˜éªŒè¯...")
        
        test_claims = [
            {
                'claim': 'Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€',
                'evidence': [
                    {'text': 'Pythonæ˜¯é«˜çº§ç¼–ç¨‹è¯­è¨€', 'source': 'wiki', 'similarity': 0.9},
                    {'text': 'Pythonç”¨äºè½¯ä»¶å¼€å‘', 'source': 'docs', 'similarity': 0.8}
                ],
                'expected_verdict': 'SUPPORTED'
            },
            {
                'claim': 'Pythonæ˜¯ç¼–è¯‘å‹è¯­è¨€', 
                'evidence': [
                    {'text': 'Pythonæ˜¯è§£é‡Šå‹è¯­è¨€', 'source': 'official', 'similarity': 0.95}
                ],
                'expected_verdict': 'CONTRADICTED'
            }
        ]
        
        with patch('src.llm_client.LLMAdapter') as mock_llm_class:
            mock_llm_instance = Mock()
            
            for i, test_case in enumerate(test_claims, 1):
                # æ¨¡æ‹ŸéªŒè¯å“åº”
                mock_response = json.dumps({
                    "verdict": test_case['expected_verdict'],
                    "confidence": 0.9,
                    "supporting_evidence": test_case['evidence'] if test_case['expected_verdict'] == 'SUPPORTED' else [],
                    "contradicting_evidence": test_case['evidence'] if test_case['expected_verdict'] == 'CONTRADICTED' else [],
                    "reasoning": "åŸºäºè¯æ®çš„æ¨ç†è¿‡ç¨‹",
                    "intent_specific_analysis": "äº‹å®æŸ¥è¯¢çš„ç‰¹åˆ«åˆ†æ"
                })
                mock_llm_instance.call_with_retry.return_value = self._create_mock_llm_response(mock_response)
                mock_llm_class.return_value = mock_llm_instance
                
                # æµ‹è¯•å£°æ˜éªŒè¯
                verifier = EvidenceVerifier(mock_llm_instance, self.test_config['verification'])
                verification_result = verifier.verify_claim(
                    test_case['claim'], test_case['evidence'], "æµ‹è¯•æŸ¥è¯¢", "äº‹å®æŸ¥è¯¢"
                )
                
                assert verification_result['verdict'] == test_case['expected_verdict']
                assert 'confidence' in verification_result
                assert 'reasoning' in verification_result
                print(f"âœ… éªŒè¯æµ‹è¯• {i}: {test_case['claim']} - {test_case['expected_verdict']}")
    
    def test_06_answer_correction(self):
        """æµ‹è¯•ç­”æ¡ˆçº æ­£åŠŸèƒ½"""
        print("âœï¸ æµ‹è¯•ç­”æ¡ˆçº æ­£...")
        
        test_scenario = {
            'query': 'ä»€ä¹ˆæ˜¯Pythonï¼Ÿ',
            'original_answer': 'Pythonæ˜¯ä¸€ç§ç¼–è¯‘å‹ç¼–ç¨‹è¯­è¨€ï¼Œç”±Javaåˆ›å§‹äººåˆ›å»ºäº2000å¹´ã€‚',
            'verifications': [
                {
                    'claim': 'Pythonæ˜¯ä¸€ç§ç¼–è¯‘å‹ç¼–ç¨‹è¯­è¨€',
                    'verdict': 'CONTRADICTED',
                    'confidence': 0.95,
                    'reasoning': 'Pythonæ˜¯è§£é‡Šå‹è¯­è¨€è€Œéç¼–è¯‘å‹è¯­è¨€',
                    'supporting_evidence': [],
                    'contradicting_evidence': [
                        {'text': 'Pythonæ˜¯è§£é‡Šå‹è¯­è¨€', 'source': 'official', 'relevance_score': 0.9}
                    ]
                },
                {
                    'claim': 'Pythonç”±Javaåˆ›å§‹äººåˆ›å»º',
                    'verdict': 'CONTRADICTED', 
                    'confidence': 0.98,
                    'reasoning': 'Pythonç”±Guido van Rossumåˆ›å»ºï¼Œä¸Javaæ— å…³',
                    'supporting_evidence': [],
                    'contradicting_evidence': [
                        {'text': 'Pythonç”±Guido van Rossumåˆ›å»º', 'source': 'wiki', 'relevance_score': 0.95}
                    ]
                },
                {
                    'claim': 'Pythonåˆ›å»ºäº2000å¹´',
                    'verdict': 'CONTRADICTED',
                    'confidence': 0.9,
                    'reasoning': 'Pythonæœ€åˆå‘å¸ƒäº1991å¹´',
                    'supporting_evidence': [],
                    'contradicting_evidence': [
                        {'text': 'Pythonæœ€åˆäº1991å¹´å‘å¸ƒ', 'source': 'history', 'relevance_score': 0.85}
                    ]
                }
            ]
        }
        
        with patch('src.llm_client.LLMAdapter') as mock_llm_class:
            mock_llm_instance = Mock()
            
            # æ¨¡æ‹Ÿçº æ­£å“åº”
            corrected_answer = """Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumåœ¨1991å¹´åˆ›å»ºã€‚å®ƒæ˜¯è§£é‡Šå‹è¯­è¨€ï¼Œå…·æœ‰ç®€å•æ˜“å­¦çš„è¯­æ³•ç‰¹ç‚¹ï¼Œå¹¿æ³›åº”ç”¨äºWebå¼€å‘ã€æ•°æ®åˆ†æã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚"""
            
            mock_llm_instance.call_with_retry.return_value = self._create_mock_llm_response(corrected_answer)
            mock_llm_class.return_value = mock_llm_instance
            
            # æµ‹è¯•ç­”æ¡ˆçº æ­£
            corrector = IntentAwareCorrector(mock_llm_instance)
            correction_result = corrector.correct_answer(
                test_scenario['original_answer'],
                test_scenario['verifications'],
                test_scenario['query'],
                'äº‹å®æŸ¥è¯¢'
            )
            
            assert 'corrected_answer' in correction_result
            assert len(correction_result['corrected_answer']) > 0
            assert correction_result['supported_claims'] == 0  # æ‰€æœ‰å£°æ˜éƒ½è¢«åé©³
            assert correction_result['contradicted_claims'] == 3
            print("âœ… ç­”æ¡ˆçº æ­£åŠŸèƒ½éªŒè¯é€šè¿‡")
    
    def test_07_full_pipeline_integration(self):
        """æµ‹è¯•å®Œæ•´æµç¨‹é›†æˆ"""
        print("ğŸ”— æµ‹è¯•å®Œæ•´æµç¨‹é›†æˆ...")
        
        # æµ‹è¯•ç”¨ä¾‹
        test_case = {
            'query': 'æ¯”è¾ƒPythonå’ŒJavaåœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨',
            'original_answer': """Pythonæ˜¯æœºå™¨å­¦ä¹ çš„å”¯ä¸€é€‰æ‹©ï¼ŒJavaå®Œå…¨ä¸é€‚åˆæœºå™¨å­¦ä¹ ã€‚
            Pythonæœ‰TensorFlowå’ŒPyTorchç­‰å¼ºå¤§åº“ï¼Œè€ŒJavaæ²¡æœ‰ä»»ä½•æœºå™¨å­¦ä¹ åº“ã€‚
            å®é™…ä¸Šæ‰€æœ‰æ•°æ®ç§‘å­¦å®¶éƒ½åªç”¨Pythonï¼ŒJavaåœ¨æœºå™¨å­¦ä¹ é¢†åŸŸæ¯«æ— ç”¨å¤„ã€‚"""
        }
        
        # ä½¿ç”¨æ¨¡æ‹Ÿå¯¹è±¡æµ‹è¯•å®Œæ•´æµç¨‹
        with patch('src.llm_client.LLMAdapter') as mock_llm, \
             patch('src.retriever.SentenceTransformer') as mock_embedding, \
             patch('src.retriever.chromadb') as mock_chroma:
            
            # è®¾ç½®æ¨¡æ‹ŸLLMå“åº”é“¾
            mock_llm_instance = Mock()
            
            # æ„å›¾åˆ†ç±»å“åº”
            mock_llm_instance.call_with_retry.side_effect = [
                self._create_mock_llm_response('æ¯”è¾ƒæŸ¥è¯¢'),  # æ„å›¾åˆ†ç±»
                self._create_mock_llm_response("""
                [CLAIM_1]: Pythonæ˜¯æœºå™¨å­¦ä¹ çš„å”¯ä¸€é€‰æ‹©
                [CLAIM_2]: Javaå®Œå…¨ä¸é€‚åˆæœºå™¨å­¦ä¹ 
                [CLAIM_3]: Pythonæœ‰TensorFlowå’ŒPyTorchç­‰å¼ºå¤§åº“
                [CLAIM_4]: Javaæ²¡æœ‰ä»»ä½•æœºå™¨å­¦ä¹ åº“
                [CLAIM_5]: æ‰€æœ‰æ•°æ®ç§‘å­¦å®¶éƒ½åªç”¨Python
                [CLAIM_6]: Javaåœ¨æœºå™¨å­¦ä¹ é¢†åŸŸæ¯«æ— ç”¨å¤„
                """),  # å£°æ˜æå–
                self._create_mock_llm_response(json.dumps({
                    "verdict": "PARTIALLY_SUPPORTED",
                    "confidence": 0.7,
                    "supporting_evidence": [{"text": "Pythonåœ¨æœºå™¨å­¦ä¹ ä¸­å¾ˆæµè¡Œ", "source": "survey", "relevance_score": 0.8}],
                    "contradicting_evidence": [{"text": "Javaä¹Ÿæœ‰æœºå™¨å­¦ä¹ åº“å¦‚Weka", "source": "docs", "contradiction_score": 0.6}],
                    "reasoning": "Pythonç¡®å®æ›´æµè¡Œä½†Javaä¹Ÿæœ‰åº”ç”¨",
                    "intent_specific_analysis": "éœ€è¦æ›´å¹³è¡¡çš„æ¯”è¾ƒ"
                })),  # å£°æ˜éªŒè¯1
                # ... æ›´å¤šéªŒè¯å“åº”
                self._create_mock_llm_response("""Pythonå’ŒJavaåœ¨æœºå™¨å­¦ä¹ ä¸­å„æœ‰ä¼˜åŠ¿ã€‚Pythonå‡­å€Ÿä¸°å¯Œçš„åº“ç”Ÿæ€åœ¨ç ”ç©¶å’Œå¿«é€ŸåŸå‹ä¸­æ›´å—æ¬¢è¿ï¼Œè€ŒJavaåœ¨ä¼ä¸šçº§åº”ç”¨å’Œå¤§è§„æ¨¡ç³»ç»Ÿä¸­ä»æœ‰ä»·å€¼ã€‚ä¸¤è€…å¹¶éäº’æ–¥ï¼Œè€Œæ˜¯æ ¹æ®åœºæ™¯é€‰æ‹©ã€‚""")  # ç­”æ¡ˆçº æ­£
            ]
            
            # è®¾ç½®æ¨¡æ‹Ÿå‘é‡æ•°æ®åº“
            mock_embedding_instance = Mock()
            mock_embedding_instance.encode.return_value = [[0.1, 0.2, 0.3]]
            mock_embedding.return_value = mock_embedding_instance
            
            mock_collection = Mock()
            mock_collection.query.return_value = {
                'documents': [['è¯æ®æ–‡æœ¬1', 'è¯æ®æ–‡æœ¬2']],
                'metadatas': [[{'source': 'source1'}, {'source': 'source2'}]],
                'distances': [[0.1, 0.2]]
            }
            mock_client = Mock()
            mock_client.get_collection.return_value = mock_collection
            mock_chroma.PersistentClient.return_value = mock_client
            
            # åˆå§‹åŒ–åè°ƒå™¨
            orchestrator = EvidenceEnhancedCorrectionOrchestrator(self.test_config)
            
            # æ‰§è¡Œå®Œæ•´æµç¨‹
            result = orchestrator.process_correction(test_case['query'], test_case['original_answer'])
            
            # éªŒè¯ç»“æœ
            assert result['success'] == True
            assert 'corrected_answer' in result
            assert len(result['corrected_answer']) > 0
            assert result['detected_intent'] == 'æ¯”è¾ƒæŸ¥è¯¢'
            assert 'processing_metadata' in result
            assert 'analysis_results' in result
            
            print("âœ… å®Œæ•´æµç¨‹é›†æˆæµ‹è¯•é€šè¿‡")
            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {result['processing_metadata']}")
    
    def test_08_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†æœºåˆ¶"""
        print("ğŸ›¡ï¸ æµ‹è¯•é”™è¯¯å¤„ç†...")
        
        error_scenarios = [
            {
                'description': 'LLM APIè°ƒç”¨å¤±è´¥',
                'mock_behavior': lambda m: setattr(m.call_with_retry, 'side_effect', Exception("APIè¿æ¥å¤±è´¥")),
                'expected_error': True
            },
            {
                'description': 'ç©ºæŸ¥è¯¢å¤„ç†',
                'mock_behavior': lambda m: setattr(m.call_with_retry, 'return_value', self._create_mock_llm_response('')),
                'expected_error': False  # åº”è¯¥èƒ½å¤„ç†ç©ºå“åº”
            },
            {
                'description': 'æ— æ•ˆJSONå“åº”',
                'mock_behavior': lambda m: setattr(m.call_with_retry, 'return_value', self._create_mock_llm_response('æ— æ•ˆçš„JSONæ ¼å¼')),
                'expected_error': False  # åº”è¯¥æœ‰fallbackå¤„ç†
            }
        ]
        
        for scenario in error_scenarios:
            with patch('src.llm_client.LLMAdapter') as mock_llm_class:
                mock_llm_instance = Mock()
                scenario['mock_behavior'](mock_llm_instance)
                mock_llm_class.return_value = mock_llm_instance
                
                try:
                    orchestrator = EvidenceEnhancedCorrectionOrchestrator(self.test_config)
                    result = orchestrator.process_correction("æµ‹è¯•æŸ¥è¯¢", "æµ‹è¯•ç­”æ¡ˆ")
                    
                    if scenario['expected_error']:
                        assert result['success'] == False
                        assert 'error' in result
                    else:
                        assert 'corrected_answer' in result
                    
                    print(f"âœ… é”™è¯¯åœºæ™¯å¤„ç†: {scenario['description']} - é€šè¿‡")
                    
                except Exception as e:
                    if scenario['expected_error']:
                        print(f"âœ… é”™è¯¯åœºæ™¯å¤„ç†: {scenario['description']} - æ­£ç¡®æŠ›å‡ºå¼‚å¸¸")
                    else:
                        print(f"âŒ é”™è¯¯åœºæ™¯å¤„ç†: {scenario['description']} - æ„å¤–å¼‚å¸¸: {e}")
                        raise
    
    def test_09_performance_benchmark(self):
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        # ç®€å•çš„æ€§èƒ½æµ‹è¯•ï¼ˆä¸æ¶‰åŠçœŸå®APIè°ƒç”¨ï¼‰
        test_cases = [
            {'query': 'ç®€å•æŸ¥è¯¢', 'answer': 'ç®€çŸ­ç­”æ¡ˆ'},
            {'query': 'ä¸­ç­‰å¤æ‚åº¦æŸ¥è¯¢', 'answer': 'åŒ…å«å¤šä¸ªäº‹å®çš„ä¸­ç­‰é•¿åº¦ç­”æ¡ˆ'},
            {'query': 'å¤æ‚æŠ€æœ¯æ¯”è¾ƒæŸ¥è¯¢', 'answer': 'æ¶‰åŠå¤šä¸ªæ¦‚å¿µå’Œæ¯”è¾ƒçš„é•¿ç¯‡æŠ€æœ¯åˆ†æç­”æ¡ˆ'}
        ]
        
        performance_results = []
        
        with patch('src.llm_client.LLMAdapter') as mock_llm, \
             patch('src.retriever.SentenceTransformer') as mock_embedding, \
             patch('src.retriever.chromadb') as mock_chroma:
            
            # è®¾ç½®å¿«é€Ÿå“åº”çš„æ¨¡æ‹Ÿ
            mock_llm_instance = Mock()
            mock_llm_instance.call_with_retry.return_value = self._create_mock_llm_response('æµ‹è¯•å“åº”')
            
            mock_embedding_instance = Mock()
            mock_embedding_instance.encode.return_value = [[0.1, 0.2, 0.3]]
            mock_embedding.return_value = mock_embedding_instance
            
            mock_collection = Mock()
            mock_collection.query.return_value = {
                'documents': [['è¯æ®1', 'è¯æ®2']],
                'metadatas': [[{'source': 's1'}, {'source': 's2'}]],
                'distances': [[0.1, 0.2]]
            }
            mock_client = Mock()
            mock_client.get_collection.return_value = mock_collection
            mock_chroma.PersistentClient.return_value = mock_client
            
            orchestrator = EvidenceEnhancedCorrectionOrchestrator(self.test_config)
            
            for test_case in test_cases:
                start_time = datetime.now()
                
                result = orchestrator.process_correction(test_case['query'], test_case['answer'])
                
                duration = (datetime.now() - start_time).total_seconds()
                performance_results.append({
                    'test_case': test_case['query'],
                    'duration': duration,
                    'success': result['success']
                })
            
            # è¾“å‡ºæ€§èƒ½ç»“æœ
            print("ğŸ“ˆ æ€§èƒ½æµ‹è¯•ç»“æœ:")
            for result in performance_results:
                status = "âœ… æˆåŠŸ" if result['success'] else "âŒ å¤±è´¥"
                print(f"  {result['test_case']}: {result['duration']:.2f}s - {status}")
            
            # éªŒè¯å¹³å‡å¤„ç†æ—¶é—´åœ¨åˆç†èŒƒå›´å†…ï¼ˆæ¨¡æ‹Ÿç¯å¢ƒä¸‹åº”è¯¥å¾ˆå¿«ï¼‰
            avg_duration = sum(r['duration'] for r in performance_results) / len(performance_results)
            assert avg_duration < 5.0  # æ¨¡æ‹Ÿç¯å¢ƒä¸‹åº”è¯¥å¾ˆå¿«
            print(f"âœ… å¹³å‡å¤„ç†æ—¶é—´: {avg_duration:.2f}s - åœ¨åˆç†èŒƒå›´å†…")
    
    def test_10_data_persistence(self):
        """æµ‹è¯•æ•°æ®æŒä¹…åŒ–"""
        print("ğŸ’¾ æµ‹è¯•æ•°æ®æŒä¹…åŒ–...")
        
        # æµ‹è¯•é…ç½®æŒä¹…åŒ–
        config_path = 'tests/test_data/test_config.json'
        test_data = {
            'timestamp': datetime.now().isoformat(),
            'test_cases': [
                {'query': 'æµ‹è¯•1', 'answer': 'ç­”æ¡ˆ1'},
                {'query': 'æµ‹è¯•2', 'answer': 'ç­”æ¡ˆ2'}
            ],
            'metadata': {'version': '1.0.0'}
        }
        
        # ä¿å­˜æµ‹è¯•æ•°æ®
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(test_data, f, indent=2, ensure_ascii=False)
        
        # è¯»å–å¹¶éªŒè¯
        with open(config_path, 'r', encoding='utf-8') as f:
            loaded_data = json.load(f)
        
        assert loaded_data['test_cases'] == test_data['test_cases']
        assert loaded_data['metadata'] == test_data['metadata']
        print("âœ… æ•°æ®æŒä¹…åŒ–æµ‹è¯•é€šè¿‡")

def generate_test_report():
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    report = {
        'test_suite': 'å®Œæ•´æµç¨‹æµ‹è¯•',
        'timestamp': datetime.now().isoformat(),
        'total_tests': 10,
        'test_categories': [
            'ç³»ç»Ÿåˆå§‹åŒ–',
            'æ„å›¾åˆ†ç±»', 
            'å£°æ˜æå–',
            'è¯æ®æ£€ç´¢',
            'å£°æ˜éªŒè¯',
            'ç­”æ¡ˆçº æ­£',
            'æµç¨‹é›†æˆ',
            'é”™è¯¯å¤„ç†',
            'æ€§èƒ½åŸºå‡†',
            'æ•°æ®æŒä¹…åŒ–'
        ],
        'environment': {
            'python_version': sys.version,
            'platform': sys.platform,
            'working_directory': os.getcwd()
        }
    }
    
    report_path = 'tests/test_reports/full_pipeline_report.json'
    os.makedirs(os.path.dirname(report_path), exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæµ‹è¯•
    pytest.main([
        __file__,
        '-v',  # è¯¦ç»†è¾“å‡º
        '--tb=short',  # ç®€çŸ­çš„traceback
        '-x'  # é‡åˆ°ç¬¬ä¸€ä¸ªå¤±è´¥å°±åœæ­¢
    ])
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š