#!/usr/bin/env python3
"""
çŸ¥è¯†åº“åˆå§‹åŒ–è„šæœ¬
ç”¨äºåŠ è½½æ–‡æ¡£ã€ç”ŸæˆåµŒå…¥å‘é‡å¹¶åˆå§‹åŒ–å‘é‡æ•°æ®åº“
"""

import os
import sys
import json
import yaml
import logging
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.retrieval.vector_retriever import VectorRetriever
from src.config.config_loader import load_config

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/knowledge_setup.log', encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)

def load_sample_documents() -> List[Dict[str, Any]]:
    """åŠ è½½ç¤ºä¾‹æ–‡æ¡£"""
    sample_docs = [
        {
            'content': 'æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“æ³¨äºç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ è€Œä¸éœ€è¦æ˜ç¡®ç¼–ç¨‹ã€‚',
            'metadata': {'source': 'wikipedia', 'type': 'definition', 'topic': 'machine_learning'}
        },
        {
            'content': 'æ·±åº¦å­¦ä¹ æ˜¯åŸºäºç¥ç»ç½‘ç»œæ¶æ„çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ ç‰¹å¾è¡¨ç¤ºï¼Œåœ¨å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­è¡¨ç°å‡ºè‰²ã€‚',
            'metadata': {'source': 'research_paper', 'type': 'technical', 'topic': 'deep_learning'}
        },
        {
            'content': 'Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumåœ¨1991å¹´åˆ›å»ºï¼Œå…·æœ‰ç®€å•æ˜“å­¦çš„è¯­æ³•ç‰¹ç‚¹ã€‚',
            'metadata': {'source': 'python_org', 'type': 'definition', 'topic': 'programming'}
        },
        {
            'content': 'ç¥ç»ç½‘ç»œå—äººè„‘ç»“æ„å¯å‘ï¼Œç”±ç›¸äº’è¿æ¥çš„èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰ç»„æˆï¼Œèƒ½å¤Ÿé€šè¿‡è®­ç»ƒå­¦ä¹ å¤æ‚æ¨¡å¼ã€‚',
            'metadata': {'source': 'textbook', 'type': 'technical', 'topic': 'neural_networks'}
        },
        {
            'content': 'è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œä¸“æ³¨äºè®¡ç®—æœºå’Œäººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ã€‚',
            'metadata': {'source': 'academic', 'type': 'definition', 'topic': 'nlp'}
        },
        {
            'content': 'Transformeræ¶æ„æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„çªç ´æ€§æŠ€æœ¯ï¼ŒåŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†åºåˆ—æ•°æ®ã€‚',
            'metadata': {'source': 'research_paper', 'type': 'technical', 'topic': 'transformer'}
        },
        {
            'content': 'å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯åŸºäºTransformeræ¶æ„çš„å¤§è§„æ¨¡ç¥ç»ç½‘ç»œï¼Œèƒ½å¤Ÿç”Ÿæˆç±»ä¼¼äººç±»çš„æ–‡æœ¬ã€‚',
            'metadata': {'source': 'tech_blog', 'type': 'definition', 'topic': 'llm'}
        },
        {
            'content': 'å¹»è§‰æ˜¯æŒ‡å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸å‡†ç¡®ã€è™šæ„æˆ–ç¼ºä¹è¯æ®æ”¯æŒçš„ä¿¡æ¯çš„ç°è±¡ã€‚',
            'metadata': {'source': 'research_paper', 'type': 'definition', 'topic': 'hallucination'}
        },
        {
            'content': 'æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡ç»“åˆæ£€ç´¢ç³»ç»Ÿå’Œç”Ÿæˆæ¨¡å‹ï¼Œå‡å°‘å¹»è§‰å¹¶æé«˜å›ç­”å‡†ç¡®æ€§ã€‚',
            'metadata': {'source': 'academic', 'type': 'technical', 'topic': 'rag'}
        },
        {
            'content': 'BERTæ˜¯Googleå¼€å‘çš„Transformeræ¨¡å‹ï¼Œé€šè¿‡åŒå‘ç¼–ç å™¨è¡¨ç¤ºå®ç°ä¸Šä¸‹æ–‡ç†è§£ã€‚',
            'metadata': {'source': 'research_paper', 'type': 'technical', 'topic': 'bert'}
        }
    ]
    return sample_docs

def load_documents_from_directory(directory_path: str) -> List[Dict[str, Any]]:
    """ä»ç›®å½•åŠ è½½æ–‡æ¡£"""
    docs = []
    directory = Path(directory_path)
    
    if not directory.exists():
        logger.warning(f"æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {directory_path}")
        return []
    
    supported_extensions = ['.txt', '.md', '.json']
    
    for file_path in directory.rglob('*'):
        if file_path.suffix.lower() in supported_extensions:
            try:
                if file_path.suffix.lower() == '.json':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = json.load(f)
                        if isinstance(content, list):
                            docs.extend(content)
                        else:
                            docs.append(content)
                else:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if content:
                            docs.append({
                                'content': content,
                                'metadata': {
                                    'source': file_path.name,
                                    'type': 'document',
                                    'file_path': str(file_path)
                                }
                            })
            except Exception as e:
                logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
    
    return docs

def initialize_knowledge_base(config: Dict[str, Any], documents: List[Dict[str, Any]]) -> bool:
    """åˆå§‹åŒ–çŸ¥è¯†åº“"""
    try:
        # åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨
        retriever = VectorRetriever(config['vector_db'])
        logger.info("âœ… å‘é‡æ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # å‡†å¤‡æ–‡æ¡£å†…å®¹
        documents_content = []
        metadatas = []
        
        for doc in documents:
            documents_content.append(doc['content'])
            metadatas.append(doc.get('metadata', {}))
        
        # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
        retriever.add_documents(documents_content, metadatas)
        logger.info(f"âœ… æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“")
        
        # éªŒè¯çŸ¥è¯†åº“çŠ¶æ€
        stats = retriever.get_collection_stats()
        logger.info(f"ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡: {stats['count']} ä¸ªæ–‡æ¡£")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    global logger
    logger = setup_logging()
    
    logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–çŸ¥è¯†åº“")
    
    try:
        # åŠ è½½é…ç½®
        config = load_config()
        logger.info("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        
        # ç¡®å®šæ–‡æ¡£æ¥æº
        knowledge_base_dir = Path('data/knowledge_base')
        documents = []
        
        if knowledge_base_dir.exists() and any(knowledge_base_dir.iterdir()):
            # ä»ç›®å½•åŠ è½½æ–‡æ¡£
            logger.info("ğŸ“ ä»çŸ¥è¯†åº“ç›®å½•åŠ è½½æ–‡æ¡£")
            documents = load_documents_from_directory('data/knowledge_base')
        else:
            # ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£
            logger.info("ğŸ“ ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£åˆå§‹åŒ–")
            documents = load_sample_documents()
            
            # åˆ›å»ºçŸ¥è¯†åº“ç›®å½•å¹¶ä¿å­˜ç¤ºä¾‹æ–‡æ¡£
            knowledge_base_dir.mkdir(parents=True, exist_ok=True)
            sample_file = knowledge_base_dir / 'sample_documents.json'
            with open(sample_file, 'w', encoding='utf-8') as f:
                json.dump(documents, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ’¾ ç¤ºä¾‹æ–‡æ¡£å·²ä¿å­˜åˆ°: {sample_file}")
        
        if not documents:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£")
            return False
        
        logger.info(f"ğŸ“„ æ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
        
        # åˆå§‹åŒ–çŸ¥è¯†åº“
        success = initialize_knowledge_base(config, documents)
        
        if success:
            logger.info("ğŸ‰ çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆ")
            return True
        else:
            logger.error("âŒ çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥")
            return False
            
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–è¿‡ç¨‹å‡ºé”™: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)