#!/usr/bin/env python3
"""
æ‰¹é‡å¤„ç†è„šæœ¬
ç”¨äºæ‰¹é‡å¤„ç†æŸ¥è¯¢æ–‡ä»¶å¹¶ç”Ÿæˆç»“æœæŠ¥å‘Š
"""

import os
import sys
import json
import csv
import time
import argparse
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.core.orchestrator import EvidenceEnhancedCorrectionOrchestrator
from src.config.config_loader import load_config

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/batch_processing.log', encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)

def load_queries_from_file(file_path: str) -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½æŸ¥è¯¢"""
    queries = []
    file_ext = Path(file_path).suffix.lower()
    
    try:
        if file_ext == '.json':
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    queries = [item.get('query', '') for item in data if item.get('query')]
                else:
                    queries = [data.get('query', '')] if data.get('query') else []
        elif file_ext == '.csv':
            with open(file_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if 'query' in row and row['query'].strip():
                        queries.append(row['query'].strip())
        else:  # txtæˆ–å…¶ä»–æ–‡æœ¬æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                queries = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        
        return [q for q in queries if q]
    
    except Exception as e:
        logger.error(f"âŒ è¯»å–æŸ¥è¯¢æ–‡ä»¶å¤±è´¥: {e}")
        return []

def save_results(results: List[Dict[str, Any]], output_format: str, output_path: str = None):
    """ä¿å­˜å¤„ç†ç»“æœ"""
    if not output_path:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"results/batch_results_{timestamp}.{output_format}"
    
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        if output_format == 'json':
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
        
        elif output_format == 'csv':
            # æå–æ‰€æœ‰å¯èƒ½çš„å­—æ®µ
            all_fields = set()
            for result in results:
                if 'results' in result:
                    all_fields.update(result['results'].keys())
                all_fields.update(result.keys())
            
            fieldnames = list(all_fields)
            with open(output_path, 'w', encoding='utf-8', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for result in results:
                    writer.writerow(result)
        
        logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        return output_path
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
        return None

def generate_report(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ç”Ÿæˆæ‰¹é‡å¤„ç†æŠ¥å‘Š"""
    successful = [r for r in results if r.get('success')]
    failed = [r for r in results if not r.get('success')]
    
    # è®¡ç®—è´¨é‡æŒ‡æ ‡
    quality_metrics = []
    processing_times = []
    
    for result in successful:
        if 'processing_metadata' in result:
            processing_times.append(result['processing_metadata'].get('total_duration', 0))
        
        if 'results' in result and 'verifications' in result['results']:
            verifications = result['results']['verifications']
            if verifications:
                supported = len([v for v in verifications if v.get('verdict') == 'SUPPORTED'])
                quality_metrics.append(supported / len(verifications))
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    avg_quality = sum(quality_metrics) / len(quality_metrics) if quality_metrics else 0
    avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
    success_rate = len(successful) / len(results) * 100 if results else 0
    
    return {
        'summary': {
            'total_queries': len(results),
            'successful_queries': len(successful),
            'failed_queries': len(failed),
            'success_rate': success_rate,
            'average_processing_time': avg_processing_time,
            'average_quality_score': avg_quality
        },
        'detailed_analysis': {
            'processing_time_stats': {
                'min': min(processing_times) if processing_times else 0,
                'max': max(processing_times) if processing_times else 0,
                'average': avg_processing_time,
                'total': sum(processing_times)
            },
            'quality_distribution': {
                'excellent': len([q for q in quality_metrics if q >= 0.9]),
                'good': len([q for q in quality_metrics if 0.7 <= q < 0.9]),
                'fair': len([q for q in quality_metrics if 0.5 <= q < 0.7]),
                'poor': len([q for q in quality_metrics if q < 0.5])
            }
        },
        'failure_analysis': {
            'total_failures': len(failed),
            'common_errors': _analyze_common_errors(failed),
            'recommendations': _generate_recommendations(failed)
        },
        'timestamp': datetime.now().isoformat()
    }

def _analyze_common_errors(failed_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """åˆ†æå¸¸è§é”™è¯¯"""
    error_counts = {}
    for result in failed_results:
        error = result.get('error', 'æœªçŸ¥é”™è¯¯')
        error_counts[error] = error_counts.get(error, 0) + 1
    
    return [{'error': error, 'count': count} for error, count in error_counts.items()]

def _generate_recommendations(failed_results: List[Dict[str, Any]]) -> List[str]:
    """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
    recommendations = []
    
    if failed_results:
        api_errors = len([r for r in failed_results if 'API' in str(r.get('error'))])
        if api_errors > 0:
            recommendations.append("æ£€æŸ¥APIå¯†é’¥é…ç½®å’Œç½‘ç»œè¿æ¥")
        
        timeout_errors = len([r for r in failed_results if 'timeout' in str(r.get('error')).lower()])
        if timeout_errors > 0:
            recommendations.append("å¢åŠ è¯·æ±‚è¶…æ—¶æ—¶é—´æˆ–å‡å°‘å¹¶å‘è¯·æ±‚æ•°")
        
        memory_errors = len([r for r in failed_results if 'memory' in str(r.get('error')).lower()])
        if memory_errors > 0:
            recommendations.append("ä¼˜åŒ–å†…å­˜ä½¿ç”¨æˆ–å¢åŠ ç³»ç»Ÿå†…å­˜")
    
    return recommendations if recommendations else ["æš‚æ— ç‰¹å®šå»ºè®®ï¼Œæ£€æŸ¥æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯"]

def process_batch(orchestrator, queries: List[str], context: str = None) -> List[Dict[str, Any]]:
    """æ‰¹é‡å¤„ç†æŸ¥è¯¢"""
    results = []
    total_queries = len(queries)
    
    logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç† {total_queries} ä¸ªæŸ¥è¯¢")
    start_time = time.time()
    
    for i, query in enumerate(queries, 1):
        logger.info(f"ğŸ“ å¤„ç†è¿›åº¦: {i}/{total_queries} - {query[:50]}...")
        
        try:
            result = orchestrator.process_query(query, context)
            result['batch_index'] = i
            result['total_queries'] = total_queries
            results.append(result)
            
            # è¿›åº¦æŠ¥å‘Š
            if i % 10 == 0 or i == total_queries:
                elapsed = time.time() - start_time
                eta = (elapsed / i) * (total_queries - i) if i > 0 else 0
                logger.info(f"ğŸ“Š è¿›åº¦: {i}/{total_queries} ({i/total_queries*100:.1f}%) - ETA: {eta:.1f}s")
                
        except Exception as e:
            error_result = {
                'success': False,
                'query': query,
                'error': str(e),
                'batch_index': i,
                'total_queries': total_queries,
                'timestamp': datetime.now().isoformat()
            }
            results.append(error_result)
            logger.error(f"âŒ æŸ¥è¯¢å¤„ç†å¤±è´¥: {query[:30]}... - {e}")
    
    total_duration = time.time() - start_time
    logger.info(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ! æ€»è€—æ—¶: {total_duration:.2f}s")
    
    return results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ‰¹é‡å¤„ç†æŸ¥è¯¢æ–‡ä»¶')
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ.txt, .json, .csvæ ¼å¼ï¼‰')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--format', '-f', choices=['json', 'csv'], default='json', help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--context', '-c', help='ä¸Šä¸‹æ–‡ä¿¡æ¯')
    parser.add_argument('--max_queries', '-m', type=int, help='æœ€å¤§å¤„ç†æŸ¥è¯¢æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    parser.add_argument('--config', default='config/config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    global logger
    logger = setup_logging()
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        logger.error(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return 1
    
    try:
        # åŠ è½½é…ç½®å’Œåˆå§‹åŒ–ç³»ç»Ÿ
        config = load_config(args.config)
        orchestrator = EvidenceEnhancedCorrectionOrchestrator(config)
        logger.info("âœ… ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        
        # åŠ è½½æŸ¥è¯¢
        queries = load_queries_from_file(args.input)
        if not queries:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æŸ¥è¯¢")
            return 1
        
        # é™åˆ¶æŸ¥è¯¢æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if args.max_queries and args.max_queries < len(queries):
            queries = queries[:args.max_queries]
            logger.info(f"ğŸ”§ é™åˆ¶å¤„ç†å‰ {args.max_queries} ä¸ªæŸ¥è¯¢")
        
        logger.info(f"ğŸ“‹ åŠ è½½åˆ° {len(queries)} ä¸ªæŸ¥è¯¢")
        
        # æ‰¹é‡å¤„ç†
        results = process_batch(orchestrator, queries, args.context)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = generate_report(results)
        
        # ä¿å­˜ç»“æœ
        output_file = save_results(results, args.format, args.output)
        
        # è¾“å‡ºæ‘˜è¦
        summary = report['summary']
        logger.info(f"""
ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!
ğŸ“Š å¤„ç†ç»Ÿè®¡:
   â€¢ æ€»æŸ¥è¯¢æ•°: {summary['total_queries']}
   â€¢ æˆåŠŸå¤„ç†: {summary['successful_queries']}
   â€¢ å¤„ç†å¤±è´¥: {summary['failed_queries']}
   â€¢ æˆåŠŸç‡: {summary['success_rate']:.1f}%
   â€¢ å¹³å‡å¤„ç†æ—¶é—´: {summary['average_processing_time']:.2f}s
   â€¢ å¹³å‡è´¨é‡åˆ†: {summary['average_quality_score']:.3f}
ğŸ’¾ ç»“æœæ–‡ä»¶: {output_file}
        """)
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())